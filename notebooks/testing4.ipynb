{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.cameras.cameras import Cameras, CameraType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iNeRFOptimizerBatchedFD:\n",
    "    def __init__(\n",
    "        self, \n",
    "        nerf_model, \n",
    "        target_image,\n",
    "        initial_pose,\n",
    "        dataparser_matrix,\n",
    "        dataparser_scale,\n",
    "        camera_params,\n",
    "        lr=0.001,\n",
    "        num_iterations=1000\n",
    "    ):\n",
    "        self.nerf_model = nerf_model\n",
    "        self.device = nerf_model.device\n",
    "        self.target_image = target_image.to(self.device)\n",
    "        \n",
    "        # Set up dataparser transforms\n",
    "        self.dataparser_matrix = torch.tensor(dataparser_matrix, dtype=torch.float32, device=self.device)\n",
    "        self.dataparser_scale = dataparser_scale\n",
    "        \n",
    "        # Set up camera parameters\n",
    "        self.camera_params = camera_params\n",
    "        \n",
    "        # Create optimizable pose parameter - just the 3x4 part of the transformation matrix\n",
    "        self.pose_param = nn.Parameter(\n",
    "            torch.tensor(initial_pose, dtype=torch.float32, device=self.device)[:3, :4].clone(),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam([self.pose_param], lr=lr)\n",
    "        self.num_iterations = num_iterations\n",
    "        self.loss_history = []\n",
    "        \n",
    "        # Debug information\n",
    "        self.timing_stats = {\n",
    "            'baseline_forward': [],\n",
    "            'batched_forward': [],\n",
    "            'gradient_calculation': [],\n",
    "            'optimization_step': []\n",
    "        }\n",
    "    \n",
    "    def optimize_step(self, batch_size=4, debug=True):\n",
    "        step_start_time = time.time()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # We'll use finite differences to compute gradients\n",
    "        eps = 1e-4  # Small epsilon for finite differences\n",
    "        \n",
    "        # Get current pose parameters\n",
    "        pose = self.pose_param.detach().clone()\n",
    "        \n",
    "        # Time the baseline forward pass\n",
    "        baseline_start = time.time()\n",
    "        original_loss, pred_rgb = self.compute_loss_no_grad(pose)\n",
    "        baseline_time = time.time() - baseline_start\n",
    "        self.timing_stats['baseline_forward'].append(baseline_time)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Baseline forward pass took {baseline_time:.4f} seconds\")\n",
    "        \n",
    "        # Time the batched gradient calculation\n",
    "        grad_start = time.time()\n",
    "        \n",
    "        # Compute gradients using finite differences in batches\n",
    "        grad = torch.zeros_like(pose)\n",
    "        \n",
    "        # Flatten the pose for easier batch processing\n",
    "        num_params = pose.numel()\n",
    "        \n",
    "        # Use coordinate indexing to track which element we're perturbing\n",
    "        coords = [(i, j) for i in range(pose.shape[0]) for j in range(pose.shape[1])]\n",
    "        \n",
    "        # Process in batches\n",
    "        batched_times = []\n",
    "        for batch_idx in range(0, num_params, batch_size):\n",
    "            batch_coords = coords[batch_idx:min(batch_idx+batch_size, num_params)]\n",
    "            \n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Create a batch of perturbed poses\n",
    "            batch_poses = []\n",
    "            for i, j in batch_coords:\n",
    "                perturbed_pose = pose.clone()\n",
    "                perturbed_pose[i, j] += eps\n",
    "                batch_poses.append(perturbed_pose)\n",
    "            \n",
    "            # Stack poses into a batch\n",
    "            batch_poses_tensor = torch.stack(batch_poses)\n",
    "            \n",
    "            # Compute losses for all poses in the batch\n",
    "            batch_losses = self.compute_batch_losses(batch_poses_tensor)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            for idx, (i, j) in enumerate(batch_coords):\n",
    "                grad[i, j] = (batch_losses[idx] - original_loss) / eps\n",
    "            \n",
    "            batch_time = time.time() - batch_start_time\n",
    "            batched_times.append(batch_time)\n",
    "            \n",
    "            if debug and len(batch_coords) > 0:\n",
    "                avg_time_per_param = batch_time / len(batch_coords)\n",
    "                processed = batch_idx + len(batch_coords)\n",
    "                remaining = num_params - processed\n",
    "                est_time_left = avg_time_per_param * remaining\n",
    "                \n",
    "                print(f\"  Processed batch {batch_idx//batch_size + 1}/{(num_params+batch_size-1)//batch_size} \" + \n",
    "                      f\"({processed}/{num_params} params, {processed/num_params*100:.1f}%) - \" + \n",
    "                      f\"Batch time: {batch_time:.4f}s - \" + \n",
    "                      f\"Est. time left: {est_time_left:.1f}s\")\n",
    "        \n",
    "        # Calculate batch statistics\n",
    "        if debug and batched_times:\n",
    "            total_batch_time = sum(batched_times)\n",
    "            self.timing_stats['batched_forward'].append(total_batch_time)\n",
    "            print(f\"Batched forward passes took {total_batch_time:.4f} seconds total\")\n",
    "            print(f\"  Speedup vs. individual forward passes: {baseline_time*num_params/total_batch_time:.2f}x\")\n",
    "            if len(batched_times) > 1:\n",
    "                print(f\"  Batch time - Min: {min(batched_times):.4f}s, Max: {max(batched_times):.4f}s, Mean: {np.mean(batched_times):.4f}s\")\n",
    "        \n",
    "        grad_time = time.time() - grad_start\n",
    "        self.timing_stats['gradient_calculation'].append(grad_time)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Gradient calculation took {grad_time:.4f} seconds\")\n",
    "            print(f\"Gradient stats - Min: {grad.min().item():.6f}, Max: {grad.max().item():.6f}, Mean: {grad.mean().item():.6f}\")\n",
    "        \n",
    "        # Manually set gradients\n",
    "        self.pose_param.grad = grad\n",
    "        \n",
    "        # Time the optimization step\n",
    "        optim_start = time.time()\n",
    "        self.optimizer.step()\n",
    "        optim_time = time.time() - optim_start\n",
    "        self.timing_stats['optimization_step'].append(optim_time)\n",
    "        \n",
    "        total_step_time = time.time() - step_start_time\n",
    "        if debug:\n",
    "            print(f\"Optimization step took {optim_time:.4f} seconds\")\n",
    "            print(f\"Total step took {total_step_time:.4f} seconds\\n\")\n",
    "        \n",
    "        return original_loss.item(), pred_rgb\n",
    "    \n",
    "    def compute_batch_losses(self, batch_poses):\n",
    "        \"\"\"Compute losses for a batch of poses\"\"\"\n",
    "        with torch.no_grad():\n",
    "            batch_size = batch_poses.shape[0]\n",
    "            batch_losses = torch.zeros(batch_size, device=self.device)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                # Create camera with the given pose\n",
    "                camera = self.create_camera_from_pose(batch_poses[i])\n",
    "                \n",
    "                # Get outputs using the model's built-in method for rendering\n",
    "                outputs = self.nerf_model.get_outputs_for_camera(camera)\n",
    "                \n",
    "                # Compute loss\n",
    "                pred_rgb, image = self.nerf_model.renderer_rgb.blend_background_for_loss_computation(\n",
    "                    pred_image=outputs[\"rgb\"],\n",
    "                    pred_accumulation=outputs[\"accumulation\"],\n",
    "                    gt_image=self.target_image,\n",
    "                )\n",
    "                \n",
    "                batch_losses[i] = self.nerf_model.rgb_loss(image, pred_rgb)\n",
    "            \n",
    "            return batch_losses\n",
    "\n",
    "    def compute_loss_no_grad(self, pose):\n",
    "        \"\"\"Compute loss without requiring gradients\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create camera with the given pose\n",
    "            camera = self.create_camera_from_pose(pose)\n",
    "            \n",
    "            # Get outputs using the model's built-in method for rendering\n",
    "            outputs = self.nerf_model.get_outputs_for_camera(camera)\n",
    "            \n",
    "            # Compute loss\n",
    "            pred_rgb, image = self.nerf_model.renderer_rgb.blend_background_for_loss_computation(\n",
    "                pred_image=outputs[\"rgb\"],\n",
    "                pred_accumulation=outputs[\"accumulation\"],\n",
    "                gt_image=self.target_image,\n",
    "            )\n",
    "            \n",
    "            loss = self.nerf_model.rgb_loss(image, pred_rgb)\n",
    "            \n",
    "        return loss, pred_rgb\n",
    "    \n",
    "    def create_camera_from_pose(self, pose):\n",
    "        \"\"\"Helper function to create a camera from pose matrix\"\"\"\n",
    "        camera = Cameras(\n",
    "            camera_to_worlds=pose.unsqueeze(0),\n",
    "            fx=self.camera_params[\"fl_x\"],\n",
    "            fy=self.camera_params[\"fl_y\"],\n",
    "            cx=self.camera_params[\"cx\"],\n",
    "            cy=self.camera_params[\"cy\"],\n",
    "            camera_type=CameraType.PERSPECTIVE,\n",
    "            height=self.camera_params[\"h\"],\n",
    "            width=self.camera_params[\"w\"],\n",
    "        )\n",
    "        return camera\n",
    "\n",
    "    def run_optimization(self, batch_size=4, visualize_every=50, debug_frequency=10):\n",
    "        best_loss = float('inf')\n",
    "        best_pose = None\n",
    "        best_image = None\n",
    "        \n",
    "        print(f\"Starting optimization with batch size {batch_size}...\")\n",
    "        overall_start_time = time.time()\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            iter_start_time = time.time()\n",
    "            \n",
    "            # Only output debug info periodically\n",
    "            debug_this_iter = (i % debug_frequency == 0)\n",
    "            \n",
    "            loss, pred_rgb = self.optimize_step(batch_size=batch_size, debug=debug_this_iter)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            iter_time = time.time() - iter_start_time\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_pose = self.pose_param.detach().clone()\n",
    "                best_image = pred_rgb.detach().clone()\n",
    "            \n",
    "            if i % visualize_every == 0 or i == self.num_iterations - 1:\n",
    "                elapsed_time = time.time() - overall_start_time\n",
    "                remaining_iters = self.num_iterations - i - 1\n",
    "                est_time_left = (iter_time * remaining_iters) if i > 0 else 0\n",
    "                \n",
    "                print(f\"Iteration {i+1}/{self.num_iterations}, Loss: {loss:.8f}\")\n",
    "                print(f\"Time for this iteration: {iter_time:.2f}s, Total elapsed: {elapsed_time:.2f}s\")\n",
    "                print(f\"Estimated time remaining: {est_time_left:.2f}s ({est_time_left/60:.2f}m)\")\n",
    "                \n",
    "                if visualize_every > 0:\n",
    "                    self.visualize_progress(i, loss, pred_rgb)\n",
    "                    \n",
    "                    # Also plot timing information\n",
    "                    if i > 0 and debug_this_iter:\n",
    "                        self.plot_timing_stats()\n",
    "        \n",
    "        total_time = time.time() - overall_start_time\n",
    "        print(f\"Optimization completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "        print(f\"Best loss: {best_loss:.8f}\")\n",
    "        \n",
    "        return best_pose, best_image, best_loss, self.loss_history\n",
    "    \n",
    "    def visualize_progress(self, iteration, loss, pred_rgb):\n",
    "        \"\"\"Visualize the current progress\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(self.target_image.detach().cpu().numpy())\n",
    "        plt.title(\"Target Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(pred_rgb.detach().cpu().numpy())\n",
    "        plt.title(f\"Current Render (Loss: {loss:.8f})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_timing_stats(self):\n",
    "        \"\"\"Plot timing statistics\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Convert lists to arrays for easier manipulation\n",
    "        baseline_times = np.array(self.timing_stats['baseline_forward'])\n",
    "        batched_times = np.array(self.timing_stats['batched_forward'])\n",
    "        grad_times = np.array(self.timing_stats['gradient_calculation'])\n",
    "        optim_times = np.array(self.timing_stats['optimization_step'])\n",
    "        \n",
    "        # Create x-axis for plots\n",
    "        x = np.arange(len(baseline_times))\n",
    "        \n",
    "        # Plot individual timing components\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(x, baseline_times, 'b-', label='Baseline Forward')\n",
    "        plt.plot(x, batched_times, 'c-', label='Batched Forward')\n",
    "        plt.plot(x, grad_times, 'r-', label='Gradient Calculation')\n",
    "        plt.plot(x, optim_times, 'g-', label='Optimizer Step')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.title('Timing Breakdown by Component')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot total time per iteration\n",
    "        plt.subplot(1, 2, 2)\n",
    "        total_times = grad_times + optim_times\n",
    "        plt.bar(x, batched_times, label='Forward Passes', color='b', alpha=0.7)\n",
    "        plt.bar(x, optim_times, bottom=batched_times, label='Optimizer Step', color='g', alpha=0.7)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.title('Total Time per Iteration')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate and print timing statistics\n",
    "        print(f\"Timing Statistics (last {len(baseline_times)} iterations):\")\n",
    "        print(f\"  Baseline Forward:      {baseline_times.mean():.4f}s (±{baseline_times.std():.4f}s)\")\n",
    "        print(f\"  Batched Forward:       {batched_times.mean():.4f}s (±{batched_times.std():.4f}s)\")\n",
    "        print(f\"  Gradient Calculation:  {grad_times.mean():.4f}s (±{grad_times.std():.4f}s)\")\n",
    "        print(f\"  Optimization Step:     {optim_times.mean():.4f}s (±{optim_times.std():.4f}s)\")\n",
    "        print(f\"  Total per Iteration:   {(batched_times+optim_times).mean():.4f}s (±{(batched_times+optim_times).std():.4f}s)\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"/n/home10/dpodolskyi/neural-registration/outputs/0_065_cat5_2/instant-ngp/2024-08-20_140044/config.yml\")\n",
    "_, pipeline, _, _ = eval_setup(config_path=config_path, test_mode=\"inference\")\n",
    "nerf_model = pipeline.model\n",
    "nerf_device = nerf_model.device\n",
    "\n",
    "# Load dataparser transforms\n",
    "dataparser_transforms_path = Path(\"/n/home10/dpodolskyi/neural-registration/outputs/0_065_cat5_2/instant-ngp/2024-08-20_140044/dataparser_transforms.json\")\n",
    "with open(dataparser_transforms_path, \"r\") as f:\n",
    "    dataparser_transform = json.load(f)\n",
    "\n",
    "dataparser_matrix = torch.tensor(dataparser_transform[\"transform\"], dtype=torch.float32)\n",
    "dataparser_matrix = np.vstack((dataparser_matrix, np.array([[0, 0, 0, 1]])))\n",
    "dataparser_scale = dataparser_transform[\"scale\"]\n",
    "\n",
    "# Initial camera pose\n",
    "initial_pose = np.array([\n",
    "    [-0.16551750084030392, 0.17173068552312126, -0.9711398089695112, -80.73672706466026],\n",
    "    [-0.9756847655052445, 0.11494663876681419, 0.18661861803471735, 15.51473463480075],\n",
    "    [0.14367740002017088, 0.9784149640546033, 0.14852933325600515, 12.348141976499424],\n",
    "    [0, 0, 0, 1],\n",
    "])\n",
    "\n",
    "# Apply dataparser transform\n",
    "final_initial_pose = np.dot(dataparser_matrix, initial_pose)\n",
    "final_initial_pose[:3, 3] = final_initial_pose[:3, 3] * dataparser_scale\n",
    "\n",
    "# Camera parameters\n",
    "camera_params = {\n",
    "    \"camera_angle_x\": 0.5235987755982988,\n",
    "    \"camera_angle_y\": 0.5235987755982988,\n",
    "    \"fl_x\": 955.4050067376327,\n",
    "    \"fl_y\": 955.4050067376327,\n",
    "    \"k1\": 0, \"k2\": 0, \"k3\": 0, \"k4\": 0, \"p1\": 0, \"p2\": 0,\n",
    "    \"is_fisheye\": False,\n",
    "    \"cx\": 256.0, \"cy\": 256.0,\n",
    "    \"w\": 512, \"h\": 512,\n",
    "    \"aabb_scale\": 32,\n",
    "}\n",
    "\n",
    "# Load target image\n",
    "target_image_path = \"/n/home10/dpodolskyi/neural-registration/data/datasets/0_065_cat5_2.0/images/4.png\"\n",
    "target_image = image_to_tensor(target_image_path, nerf_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option 3: Batched finite differences (compromise solution)\n",
    "# inerf_optimizer = iNeRFOptimizerBatchedFD(\n",
    "#     nerf_model=nerf_model,\n",
    "#     target_image=target_image,\n",
    "#     initial_pose=final_initial_pose,\n",
    "#     dataparser_matrix=dataparser_matrix,\n",
    "#     dataparser_scale=dataparser_scale,\n",
    "#     camera_params=camera_params,\n",
    "#     lr=0.001,\n",
    "#     num_iterations=1000\n",
    "# )\n",
    "# batch_size = 4  # Adjust based on your GPU memory\n",
    "# best_pose, best_image, best_loss, loss_history = inerf_optimizer.run_optimization(batch_size=batch_size, visualize_every=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
