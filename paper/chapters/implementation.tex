\chapter{Implementation}

This chapter details the technical implementation of our NeRF-based intraoperative registration framework. We describe the software architecture, the specific NeRF implementation used, the optimization algorithms, and the implementation of the various loss functions and hypernetwork modules.

\section{Software Architecture}
Our implementation is built on top of nerfstudio \cite{tancik2023nerfstudio}, a modular framework for Neural Radiance Field development. Nerfstudio provides abstractions for NeRF training, rendering, and evaluation, allowing us to focus on the registration-specific components of our framework.

\subsection{System Overview}
The system consists of the following components:

\begin{itemize}
    \item \textbf{NeRF Model}: A pre-trained Neural Radiance Field that represents the brain surface.
    \item \textbf{Renderer}: Responsible for generating images from the NeRF given a camera pose.
    \item \textbf{Pose Optimizer}: Optimizes the camera pose to align the rendered image with the target image.
    \item \textbf{Loss Module}: Computes the similarity between the rendered and target images.
    \item \textbf{Hypernetwork Module}: Generates style parameters to adapt the NeRF rendering to match the target modality.
\end{itemize}

\subsection{Dependency Management}
The implementation relies on the following key dependencies:

\begin{itemize}
    \item \textbf{PyTorch}: For neural network implementation and automatic differentiation.
    \item \textbf{Nerfstudio}: For NeRF abstractions and rendering.
    \item \textbf{Kornia}: For differentiable computer vision operations.
    \item \textbf{NumPy}: For numerical computations.
    \item \textbf{OpenCV}: For image processing.
\end{itemize}

\section{NeRF Implementation}
Our framework is designed to be agnostic to the specific NeRF implementation, allowing for experimentation with different architectures. For our experiments, we primarily use the Nerfacto model provided by nerfstudio, which combines features from several state-of-the-art NeRF variants.

\subsection{NeRF Training}
The NeRF is trained on preoperative MRI data using the standard nerfstudio training pipeline. The training process involves:

\begin{enumerate}
    \item Preprocessing the MRI data to extract surface information.
    \item Converting the MRI volumes to a set of posed images.
    \item Training the NeRF model to reconstruct these images.
\end{enumerate}

\subsection{Rendering Pipeline}
The rendering pipeline follows the standard NeRF approach:

\begin{enumerate}
    \item For each pixel in the output image, cast a ray from the camera origin through the pixel.
    \item Sample points along the ray.
    \item Evaluate the NeRF at each sample point to obtain density and color values.
    \item Perform volume rendering to compute the final pixel color.
\end{enumerate}

We extend this pipeline to incorporate the hypernetwork-generated style parameters, modulating the NeRF output before the volume rendering step.

\section{Pose Optimization}
The pose optimization module is responsible for finding the camera pose that best aligns the rendered NeRF view with the target intraoperative image.

\subsection{Pose Parameterization}
We parameterize the camera pose using a 6-degree-of-freedom (6DoF) representation:

\begin{itemize}
    \item 3 parameters for translation $(t_x, t_y, t_z)$
    \item 3 parameters for rotation, represented as Euler angles $(\alpha, \beta, \gamma)$ or quaternions
\end{itemize}

To ensure stable optimization, we normalize the translation parameters to a consistent scale and use a continuous representation for rotations to avoid discontinuities.

\subsection{Optimization Algorithm}
We implement several optimization algorithms to compare their performance:

\begin{itemize}
    \item \textbf{Gradient Descent}: Simple first-order optimization with a fixed or adaptive learning rate.
    \item \textbf{Adam}: Adaptive moment estimation, which adapts the learning rate for each parameter based on the history of gradients.
    \item \textbf{L-BFGS}: A quasi-Newton method that approximates the Hessian matrix to achieve faster convergence.
\end{itemize}

The optimization process is implemented using PyTorch's automatic differentiation capabilities, allowing us to compute gradients of the loss function with respect to the pose parameters efficiently.

\subsection{Convergence Criteria}
We use the following criteria to determine when the optimization has converged:

\begin{itemize}
    \item Maximum number of iterations reached
    \item Loss value below a specified threshold
    \item Gradient magnitude below a specified threshold
    \item No significant improvement in loss for a specified number of iterations
\end{itemize}

\section{Loss Function Implementation}
We implement the loss functions described in the methodology chapter, ensuring that they are differentiable with respect to the pose parameters.

\subsection{Pixel-wise Losses}
The L2 loss and weighted L2 loss are implemented using PyTorch's built-in functions:

\begin{lstlisting}[language=Python]
def l2_loss(rendered_img, target_img):
    return torch.mean((rendered_img - target_img) ** 2)

def weighted_l2_loss(rendered_img, target_img, weights):
    return torch.mean(weights * (rendered_img - target_img) ** 2)
\end{lstlisting}

\subsection{Structural Losses}
For the normalized cross-correlation and SSIM, we use implementations from the Kornia library, which provides differentiable versions of these metrics:

\begin{lstlisting}[language=Python]
def ncc_loss(rendered_img, target_img):
    # Normalize images
    rendered_norm = (rendered_img - rendered_img.mean()) / rendered_img.std()
    target_norm = (target_img - target_img.mean()) / target_img.std()
    
    # Compute NCC
    return -torch.mean(rendered_norm * target_norm)

def ssim_loss(rendered_img, target_img):
    return 1.0 - kornia.losses.ssim_loss(
        rendered_img, target_img, window_size=11, reduction='mean'
    )
\end{lstlisting}

\subsection{Information-Theoretic Losses}
The mutual information loss is implemented using histogram-based approximations:

\begin{lstlisting}[language=Python]
def mutual_information_loss(rendered_img, target_img, num_bins=20):
    # Compute joint histogram
    joint_hist = compute_joint_histogram(rendered_img, target_img, num_bins)
    
    # Compute marginal histograms
    hist_rendered = torch.sum(joint_hist, dim=1)
    hist_target = torch.sum(joint_hist, dim=0)
    
    # Compute mutual information
    eps = 1e-10  # Small constant to avoid log(0)
    joint_p = joint_hist / torch.sum(joint_hist)
    p_rendered = hist_rendered / torch.sum(hist_rendered)
    p_target = hist_target / torch.sum(hist_target)
    
    mi = torch.sum(joint_p * torch.log(joint_p / (p_rendered.unsqueeze(1) * p_target.unsqueeze(0)) + eps))
    
    # Return negative MI as we want to maximize MI
    return -mi
\end{lstlisting}

\section{Hypernetwork Implementation}
The hypernetwork module generates style parameters to adapt the NeRF rendering to match the target modality.

\subsection{Hypernetwork Architecture}
We implement the hypernetwork as a multi-layer perceptron (MLP) that takes a style code as input and outputs modulation parameters for the NeRF:

\begin{lstlisting}[language=Python]
class Hypernetwork(nn.Module):
    def __init__(self, style_dim, output_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(style_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim)
        )
    
    def forward(self, style_code):
        return self.network(style_code)
\end{lstlisting}

\subsection{Style Encoding}
We implement various methods for encoding the style information:

\begin{lstlisting}[language=Python]
def yuv_encoding(image):
    # Convert RGB to YUV
    yuv_image = kornia.color.rgb_to_yuv(image)
    # Extract statistics (mean and std) for each channel
    mean = torch.mean(yuv_image, dim=[2, 3])
    std = torch.std(yuv_image, dim=[2, 3])
    return torch.cat([mean, std], dim=1)

def hog_encoding(image):
    # Compute HOG features
    hog_features = kornia.feature.hog(
        image, orientation=8, cell_size=(8, 8), 
        cast_long_to_float=True
    )
    # Pool features to get a compact representation
    pooled_features = F.adaptive_avg_pool2d(hog_features, (1, 1)).squeeze()
    return pooled_features

def gram_matrix_encoding(features):
    b, c, h, w = features.size()
    features_reshaped = features.view(b, c, h * w)
    gram = torch.bmm(features_reshaped, features_reshaped.transpose(1, 2))
    return gram / (c * h * w)
\end{lstlisting}

\subsection{Style Modulation}
We implement several methods for applying the style parameters to the NeRF output:

\begin{lstlisting}[language=Python]
def adaptive_instance_normalization(content_feat, style_params):
    # Extract style parameters
    style_mean, style_std = style_params.chunk(2, dim=1)
    
    # Normalize content features
    content_mean = torch.mean(content_feat, dim=[2, 3], keepdim=True)
    content_std = torch.std(content_feat, dim=[2, 3], keepdim=True) + 1e-5
    normalized = (content_feat - content_mean) / content_std
    
    # Apply style
    return normalized * style_std.unsqueeze(2).unsqueeze(3) + style_mean.unsqueeze(2).unsqueeze(3)

def feature_modulation(nerf_output, style_params):
    # Apply modulation to NeRF output
    rgb, density = nerf_output
    
    # Extract modulation parameters
    rgb_scale, rgb_bias, density_scale, density_bias = style_params.chunk(4, dim=1)
    
    # Apply modulation
    modulated_rgb = rgb * rgb_scale.unsqueeze(1) + rgb_bias.unsqueeze(1)
    modulated_density = density * density_scale.unsqueeze(1) + density_bias.unsqueeze(1)
    
    return modulated_rgb, modulated_density
\end{lstlisting}

\section{Experimental Setup}
We implement a comprehensive experimental framework to evaluate our registration approach.

\subsection{Data Generation}
For controlled experiments, we generate synthetic data from the pre-trained NeRF:

\begin{lstlisting}[language=Python]
def generate_target_image(nerf_model, target_pose):
    # Render image from the NeRF at the target pose
    with torch.no_grad():
        target_image = nerf_model.render(target_pose)
    return target_image

def generate_dataset(nerf_model, num_samples, pose_range):
    dataset = []
    for _ in range(num_samples):
        # Sample a random target pose
        target_pose = sample_random_pose(pose_range)
        # Generate target image
        target_image = generate_target_image(nerf_model, target_pose)
        # Sample an initial pose (perturbed from target)
        initial_pose = perturb_pose(target_pose, perturbation_range)
        dataset.append({
            'target_image': target_image,
            'target_pose': target_pose,
            'initial_pose': initial_pose
        })
    return dataset
\end{lstlisting}

\subsection{Evaluation Metrics}
We implement several metrics to evaluate the registration accuracy:

\begin{lstlisting}[language=Python]
def compute_pose_error(estimated_pose, ground_truth_pose):
    # Compute translation error
    trans_error = torch.norm(estimated_pose[:3] - ground_truth_pose[:3])
    
    # Compute rotation error (in degrees)
    rot_error = compute_rotation_error(
        estimated_pose[3:], ground_truth_pose[3:]
    )
    
    return trans_error, rot_error

def compute_image_similarity(rendered_img, target_img):
    # Compute various similarity metrics
    mse = torch.mean((rendered_img - target_img) ** 2)
    psnr = -10 * torch.log10(mse)
    ssim = kornia.metrics.ssim(rendered_img, target_img, window_size=11)
    
    return {
        'mse': mse.item(),
        'psnr': psnr.item(),
        'ssim': ssim.item()
    }
\end{lstlisting}

\subsection{Visualization Tools}
We implement visualization tools to analyze the registration process:

\begin{lstlisting}[language=Python]
def visualize_registration(nerf_model, initial_pose, estimated_pose, 
                          target_pose, target_image):
    # Render images from different poses
    initial_image = nerf_model.render(initial_pose)
    estimated_image = nerf_model.render(estimated_pose)
    ground_truth_image = nerf_model.render(target_pose)
    
    # Create visualization grid
    grid = torch.cat([
        initial_image, estimated_image, 
        ground_truth_image, target_image
    ], dim=2)
    
    return grid

def plot_convergence(loss_history, pose_error_history):
    # Plot loss and pose error over iterations
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
    
    # Plot loss
    ax1.plot(loss_history)
    ax1.set_ylabel('Loss')
    ax1.set_title('Loss Convergence')
    
    # Plot pose error
    ax2.plot(pose_error_history['translation'], label='Translation Error')
    ax2.plot(pose_error_history['rotation'], label='Rotation Error')
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Error')
    ax2.set_title('Pose Error Convergence')
    ax2.legend()
    
    return fig
\end{lstlisting}

In the next chapter, we present the experimental results obtained using this implementation, comparing the performance of different loss functions and hypernetwork approaches for intraoperative registration. 