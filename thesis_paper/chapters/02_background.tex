% !TeX root = ../main.tex

\chapter{Theoretical Background and Related Work}\label{chapter:background}

%This chapter provides the necessary theoretical foundation and contextual background for understanding the work presented in this thesis. We first review the field of intraoperative registration, followed by an overview of Neural Radiance Fields (NeRFs) and their applications in pose estimation. Finally, we discuss relevant work on loss functions for image registration and style transfer techniques.

\section{Neural Networks and Backpropagation}

Neural Radiance Field (NeRF) models are fundamentally based on artificial neural networks (ANNs). ANNs are computational models comprising interconnected processing units or "neurons," arranged in layers that transform input data through a series of non-linear functions to produce output predictions\parencite{Han2018Artificial}. Formally, each neuron computes a weighted sum of its inputs, applies a non-linear activation function $\sigma(\cdot)$, and passes the result to subsequent neurons:

\begin{equation}
y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right)
\end{equation}

\noindent where $w_i$ represents the weights, $x_i$ the inputs, and $b$ the bias term.

Neural networks optimize their parameters through gradient-based learning algorithms, predominantly backpropagation coupled with stochastic gradient descent (SGD) or its variants. The training process involves forward propagation of input data through the network, yielding predictions that are evaluated against ground truth via a differentiable loss function $\mathcal{L}(\theta)$, where $\theta$ represents the network parameters. Backpropagation then computes the gradient $\nabla_\theta \mathcal{L}(\theta)$ by recursively applying the chain rule of differentiation to determine each parameter's contribution to the total error.

Parameter updates proceed iteratively according to:

\begin{equation}
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
\end{equation}

\noindent where $\eta$ denotes the learning rate that governs the magnitude of parameter adjustments. This optimization process continues over multiple epochs as the network processes mini-batches of training data, progressively minimizing the objective function and improving the model's predictive performance.

\section{Intraoperative Registration in Neurosurgery}

Intraoperative registration is an essential component of neurosurgical procedures, involving the precise alignment of preoperative imaging data with the patient's actual anatomy during surgery. The primary goal of intraoperative registration is to create an accurate spatial correspondence between the preoperative images — usually Magnetic Resonance Imaging (MRI) — and the physical patient. \textcite{fehrentz2024intraoperative} This alignment ensures surgeons can reliably utilize the rich anatomical information provided by preoperative images, thereby improving surgical accuracy, reducing complications, and enhancing patient safety.

Various registration methods have been developed to achieve this alignment, ranging from traditional approaches, such as point-based, surface-based, and volume-based techniques, to more complex methods like cross-modal registration. These techniques differ significantly in terms of their computational complexity, equipment requirements, accuracy, and adaptability to intraoperative changes such as brain shift and tissue deformation.
\subsection{Traditional Registration Approaches}

Traditional approaches to intraoperative registration in neurosurgery can be broadly categorized into the following methods:

\begin{itemize}
    \item \textbf{Point-based registration}: This approach identifies and matches corresponding anatomical landmarks or artificially placed fiducial markers in both the preoperative images and physical patient. While conceptually simple, it requires accurate identification of landmarks and can be time-consuming. \parencite{Fitzpatrick1998Predicting}
    
    \item \textbf{Surface-based registration}: This technique matches surfaces extracted from preoperative imaging with surfaces captured intraoperatively, often using techniques like the Iterative Closest Point (ICP) algorithm. Surface-based approaches typically require specialized equipment, such as laser scanners or stereo cameras, to capture the intraoperative surface. \parencite{CLARKSON2011856}
    
    \item \textbf{Volume-based registration}: These methods use intensity-based similarity measures to align volumetric images, but they typically require intraoperative imaging modalities such as ultrasound or intraoperative MRI, which may not be available in all surgical settings. \parencite{Klein2010Evaluation}
\end{itemize}

A significant challenge in neurosurgical registration is brain shift, the deformation of brain tissue that occurs during surgery due to factors such as gravity, cerebrospinal fluid drainage, and surgical manipulations. This phenomenon can significantly reduce the accuracy of rigid registration methods and necessitates more advanced techniques.

\subsection{Cross-Modal Registration}

Cross-modal registration refers to the alignment of images from different imaging modalities. In the context of neurosurgery, this often involves aligning preoperative MRI data with intraoperative camera images. This presents unique challenges due to differences in:

\begin{itemize}
    \item \textbf{Information content}: MRI provides volumetric data with tissue contrast, while optical images capture surface appearance with details like blood vessels and lighting effects. \parencite{Choe2011Accuracy}
    \item \textbf{Geometric representation}: MRI data is three-dimensional, while camera images are two-dimensional projections. \parencite{Unberath2021The}
    \item \textbf{Appearance}: The visual appearance of tissues differs significantly between MRI and optical images due to different physical principles of image formation. \parencite{Xie2023Cross}
\end{itemize}

Previous work in cross-modal registration has employed techniques such as feature extraction, mutual information maximization, and deep learning-based approaches to bridge these differences.

\section{Neural Radiance Fields (NeRFs)}

Neural Radiance Fields, introduced by \textcite{mildenhall2020nerf}, represent a novel approach to scene representation and novel view synthesis. Unlike traditional computer graphics methods that use explicit representations like meshes or point clouds, NeRFs employ an implicit neural representation to model scenes.

\subsection{NeRF Representation}

A NeRF is typically implemented as a multi-layer perceptron (MLP) that maps a 3D coordinate $\mathbf{x} = (x, y, z)$ and viewing direction $\mathbf{d} = (\theta, \phi)$ to a color $\mathbf{c} = (r, g, b)$ and volume density $\sigma$:

\begin{equation}
    F_\Theta: (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
\end{equation}

where $\Theta$ represents the learnable parameters of the neural network. This continuous, differentiable representation allows for rendering from arbitrary viewpoints through volume rendering techniques.

The rendering process involves casting rays from a camera through image pixels and evaluating the NeRF at multiple points along each ray. The color of a pixel is computed as a weighted sum of the colors along the ray, with weights determined by the volume densities:

\begin{equation}
    C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(t) \mathbf{c}(t) dt
\end{equation}

where $T(t) = \exp\left(-\int_{t_n}^{t} \sigma(s) ds\right)$ represents the accumulated transmittance along the ray up to point $t$.

\subsection{NeRF Variants}

Since the introduction of the original NeRF, numerous variants have been developed to address limitations and extend capabilities:

\begin{itemize}
    \item \textbf{Instant-NGP} \parencite{muller2022instant}: Accelerates NeRF training and rendering through multi-resolution hash encoding, reducing training time from days to minutes.
    
    \item \textbf{HyperNeRF} \parencite{wang2023hypernerf}: Extends NeRFs to handle topological variations in dynamic scenes through a higher-dimensional representation.
    
    \item \textbf{Nerfacto} \parencite{Tancik_2023}: An implementation-agnostic framework that combines advances from various NeRF variants for improved performance.
\end{itemize}

\subsection{iNeRF: Inverting Neural Radiance Fields for Pose Estimation}

\textcite{yen2020inerf} introduced iNeRF, a method that leverages the differentiable nature of NeRFs for pose estimation. Given a target image and a pre-trained NeRF, iNeRF estimates the camera pose from which the target image was captured. This is achieved by optimizing the camera pose parameters to minimize the difference between the rendered image (from the current pose estimate) and the target image.

The key insight of iNeRF is that the camera pose can be optimized through backpropagation, utilizing the differentiable nature of both the NeRF representation and the rendering process. This optimization is formulated as:

\begin{equation}
    \hat{\xi} = \arg\min_{\xi} \mathcal{L}(I_{\text{target}}, I_{\text{rendered}}(\xi))
\end{equation}

where $\xi$ represents the camera pose parameters, $I_{\text{target}}$ is the target image, $I_{\text{rendered}}(\xi)$ is the image rendered from the NeRF using pose $\xi$, and $\mathcal{L}$ is a loss function measuring the dissimilarity between the images.

\section{Cross-Modal Inverse Neural Rendering for Registration}

Building on the concept of iNeRF, \textcite{fehrentz2024intraoperative} proposed a method for intraoperative registration using cross-modal inverse neural rendering. Their approach addresses the challenge of cross-modal registration by separating the neural representation into structural and appearance components:

\begin{itemize}
    \item The \textbf{structural component} captures the geometric properties of the brain and is learned from preoperative MRI data.
    
    \item The \textbf{appearance component} is adapted intraoperatively to match the visual characteristics of surgical images.
\end{itemize}

This separation is achieved through a multi-style hypernetwork that controls the appearance of the NeRF while preserving its learned representation of the anatomy. The hypernetwork generates parameters for a subset of the NeRF's layers, allowing it to produce different appearances for the same underlying geometry.

During registration, the approach optimizes both the camera pose and the appearance parameters to minimize the dissimilarity between the rendered and target intraoperative images. This method has shown promising results in clinical data, outperforming state-of-the-art methods while meeting clinical standards for registration accuracy.

\section{Loss Functions for Image Registration}

The choice of loss function is crucial in registration tasks, as it defines the measure of similarity between images that guides the optimization process. Different loss functions capture different aspects of image similarity and may be more or less suitable depending on the specific registration task.

\subsection{L2 Loss}

The L2 loss, or mean squared error (MSE), is commonly used in image registration tasks due to its simplicity and differentiability. It calculates the squared Euclidean distance between two images:

\begin{equation}
    \mathcal{L}_{\text{L2}}(I_1, I_2) = \frac{1}{N} \sum_{i=1}^{N} (I_1(i) - I_2(i))^2
\end{equation}

where $N$ is the number of pixels. While straightforward, L2 loss assumes a direct intensity correspondence between images, which may not hold in cross-modal scenarios.

\subsection{Normalized Cross-Correlation (NCC)}

Normalized Cross-Correlation measures the similarity between two images independently of linear intensity transformations:

\begin{equation}
    \mathcal{L}_{\text{NCC}}(I_1, I_2) = -\frac{\sum_{i=1}^{N} (I_1(i) - \bar{I}_1)(I_2(i) - \bar{I}_2)}{\sqrt{\sum_{i=1}^{N} (I_1(i) - \bar{I}_1)^2 \sum_{i=1}^{N} (I_2(i) - \bar{I}_2)^2}}
\end{equation}

where $\bar{I}_1$ and $\bar{I}_2$ are the mean intensities of the respective images. NCC is particularly useful when images have different contrast or brightness levels \parencite{nccreg}.

\subsection{Mutual Information (MI)}

Mutual Information is a statistical measure that quantifies the mutual dependence between two random variables, making it particularly suitable for cross-modal registration where the relationship between intensities is complex:

\begin{equation}
    \mathcal{L}_{\text{MI}}(I_1, I_2) = -\sum_{i,j} p_{I_1,I_2}(i,j) \log\left(\frac{p_{I_1,I_2}(i,j)}{p_{I_1}(i)p_{I_2}(j)}\right)
\end{equation}

where $p_{I_1,I_2}$ is the joint probability distribution of intensities in images $I_1$ and $I_2$, and $p_{I_1}$ and $p_{I_2}$ are their marginal distributions \parencite{mi2003}.

\subsection{Weighted and Masked L2 Loss}

Weighted and masked variants of the L2 loss assign different importance to different regions of the image:

\begin{equation}
    \mathcal{L}_{\text{wL2}}(I_1, I_2) = \frac{1}{N} \sum_{i=1}^{N} w(i) (I_1(i) - I_2(i))^2
\end{equation}

where $w(i)$ is a weight or mask value for pixel $i$. This approach can be useful for focusing the registration on regions of interest or for ignoring irrelevant areas.