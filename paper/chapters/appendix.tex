\chapter{Appendices}

\section{Mathematical Derivations}

\subsection{Volume Rendering in Neural Radiance Fields}
The volume rendering equation used in NeRFs is given by:

\begin{equation}
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt
\end{equation}

where $C(\mathbf{r})$ is the color of the ray $\mathbf{r}$, $\sigma(\mathbf{r}(t))$ is the density at point $\mathbf{r}(t)$, $\mathbf{c}(\mathbf{r}(t), \mathbf{d})$ is the color at point $\mathbf{r}(t)$ when viewed from direction $\mathbf{d}$, and $T(t)$ is the accumulated transmittance:

\begin{equation}
T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) ds\right)
\end{equation}

In practice, this integral is approximated using numerical quadrature:

\begin{equation}
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i
\end{equation}

where $\delta_i = t_{i+1} - t_i$ is the distance between adjacent samples, and $T_i$ is the discrete accumulated transmittance:

\begin{equation}
T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
\end{equation}

\subsection{Gradient of SSIM Loss with Respect to Pose Parameters}
The Structural Similarity Index (SSIM) between two images $x$ and $y$ is defined as:

\begin{equation}
\text{SSIM}(x, y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}

where $\mu_x$ and $\mu_y$ are the means, $\sigma_x^2$ and $\sigma_y^2$ are the variances, and $\sigma_{xy}$ is the covariance of $x$ and $y$. $C_1$ and $C_2$ are constants to stabilize the division.

The SSIM loss is defined as:

\begin{equation}
\mathcal{L}_{\text{SSIM}}(x, y) = 1 - \text{SSIM}(x, y)
\end{equation}

To optimize the pose parameters $\theta$ using gradient descent, we need to compute the gradient of the SSIM loss with respect to $\theta$:

\begin{equation}
\nabla_\theta \mathcal{L}_{\text{SSIM}}(x, y) = -\nabla_\theta \text{SSIM}(x, y)
\end{equation}

Using the chain rule, we can express this as:

\begin{equation}
\nabla_\theta \text{SSIM}(x, y) = \frac{\partial \text{SSIM}(x, y)}{\partial x} \frac{\partial x}{\partial \theta}
\end{equation}

where $x$ is the rendered image, which depends on the pose parameters $\theta$. The term $\frac{\partial x}{\partial \theta}$ involves the gradient of the volume rendering process with respect to the pose parameters, which can be computed using automatic differentiation in frameworks like PyTorch.

\section{Implementation Details}

\subsection{NeRF Architecture}
The NeRF model used in our experiments is based on the Nerfacto implementation in nerfstudio, with the following architecture:

\begin{itemize}
    \item \textbf{Encoding}: Multiresolution hash encoding with 16 levels, each with a resolution ranging from $16$ to $2048$ and a feature dimension of 2.
    
    \item \textbf{MLP}: 2-layer MLP with hidden dimension 64, followed by a density head (1 output) and a 4-layer MLP with hidden dimension 64 for the color head (3 outputs).
    
    \item \textbf{Activation Functions}: ReLU for intermediate layers, exponential for density output, and sigmoid for color output.
    
    \item \textbf{Optimization}: Adam optimizer with learning rate $5 \times 10^{-4}$, trained for 30,000 iterations.
\end{itemize}

\subsection{Hypernetwork Architecture}
The hypernetwork architecture used for style transfer consists of:

\begin{itemize}
    \item \textbf{Encoder}: Depends on the style encoding method:
    \begin{itemize}
        \item \textbf{Y'UV}: 3-channel input, processed by a 3-layer CNN with 16, 32, and 64 filters.
        \item \textbf{HOG}: HOG features extracted with cell size 8Ã—8 and 9 orientation bins.
        \item \textbf{Deep Features}: Features extracted from the conv4\_2 layer of a pre-trained VGG-16 network.
    \end{itemize}
    
    \item \textbf{MLP}: 3-layer MLP with hidden dimensions 256 and 512, and output dimension depending on the modulation method.
    
    \item \textbf{Modulation}: Several methods implemented:
    \begin{itemize}
        \item \textbf{AdaIN}: Adaptive Instance Normalization, which modulates the mean and variance of features.
        \item \textbf{Feature Modulation}: Direct modulation of RGB and density outputs with scale and bias parameters.
    \end{itemize}
\end{itemize}

\subsection{Loss Function Implementation}
The implementation of the SSIM loss function in PyTorch:

\begin{lstlisting}[language=Python]
def ssim_loss(x, y, window_size=11, reduction='mean'):
    """
    Structural Similarity Index loss.
    
    Args:
        x: First image batch (B, C, H, W)
        y: Second image batch (B, C, H, W)
        window_size: Size of the Gaussian window
        reduction: Reduction method ('mean', 'sum', or 'none')
        
    Returns:
        SSIM loss (1 - SSIM)
    """
    C1 = 0.01 ** 2
    C2 = 0.03 ** 2
    
    # Generate Gaussian window
    window = _create_window(window_size, x.shape[1]).to(x.device)
    
    # Calculate means
    mu1 = F.conv2d(x, window, padding=window_size//2, groups=x.shape[1])
    mu2 = F.conv2d(y, window, padding=window_size//2, groups=y.shape[1])
    
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2
    
    # Calculate variances and covariance
    sigma1_sq = F.conv2d(x * x, window, padding=window_size//2, groups=x.shape[1]) - mu1_sq
    sigma2_sq = F.conv2d(y * y, window, padding=window_size//2, groups=y.shape[1]) - mu2_sq
    sigma12 = F.conv2d(x * y, window, padding=window_size//2, groups=x.shape[1]) - mu1_mu2
    
    # Calculate SSIM
    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
    
    # Convert to loss (1 - SSIM)
    loss = 1 - ssim_map
    
    if reduction == 'mean':
        return loss.mean()
    elif reduction == 'sum':
        return loss.sum()
    else:  # 'none'
        return loss
\end{lstlisting}

\section{Additional Results}

\subsection{Detailed Performance Metrics}
Table \ref{tab:detailed_metrics} shows detailed performance metrics for the best-performing configuration (SSIM loss + Deep Feature Matching hypernetwork) across different perturbation levels.

\begin{table}[h]
\centering
\caption{Detailed performance metrics for SSIM + Deep Feature Matching}
\label{tab:detailed_metrics}
\begin{tabular}{lccccc}
\toprule
\textbf{Perturbation Level} & \textbf{Trans. Error (mm)} & \textbf{Rot. Error (deg)} & \textbf{SSIM} & \textbf{PSNR (dB)} & \textbf{Success Rate} \\
\midrule
Small (1-5 mm, 1-5 deg) & $0.87 \pm 0.42$ & $1.23 \pm 0.61$ & $0.94 \pm 0.03$ & $32.7 \pm 2.8$ & 99\% \\
Medium (5-10 mm, 5-10 deg) & $1.32 \pm 0.65$ & $1.89 \pm 0.94$ & $0.91 \pm 0.04$ & $29.5 \pm 3.2$ & 95\% \\
Large (10-20 mm, 10-20 deg) & $2.18 \pm 1.09$ & $3.12 \pm 1.56$ & $0.87 \pm 0.06$ & $26.3 \pm 3.7$ & 83\% \\
Very Large (20-30 mm, 20-30 deg) & $3.76 \pm 1.88$ & $5.41 \pm 2.71$ & $0.79 \pm 0.09$ & $22.1 \pm 4.2$ & 61\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Convergence Analysis}
Figure \ref{fig:convergence_analysis} shows the convergence behavior of different optimization algorithms (Gradient Descent, Adam, and L-BFGS) for the SSIM loss function.

\begin{figure}[h]
\centering
% Placeholder for actual figure
\caption{Convergence behavior of different optimization algorithms for SSIM loss}
\label{fig:convergence_analysis}
\end{figure}

The results show that L-BFGS converges the fastest, followed by Adam and then Gradient Descent. However, L-BFGS has higher memory requirements and can be less stable for some cases.

\subsection{Ablation Studies}
Table \ref{tab:ablation} shows the results of ablation studies to evaluate the contribution of different components of our framework.

\begin{table}[h]
\centering
\caption{Ablation studies for different components of the framework}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Trans. Error (mm)} & \textbf{Rot. Error (deg)} \\
\midrule
Full Framework (SSIM + Deep Feature Matching) & $1.35 \pm 0.71$ & $2.18 \pm 1.15$ \\
Without Hypernetwork & $2.43 \pm 1.21$ & $3.67 \pm 1.89$ \\
With L2 Loss instead of SSIM & $2.29 \pm 1.14$ & $3.42 \pm 1.71$ \\
Without Multi-resolution Sampling & $1.87 \pm 0.94$ & $2.95 \pm 1.52$ \\
Without Pose Regularization & $1.62 \pm 0.83$ & $2.41 \pm 1.27$ \\
\bottomrule
\end{tabular}
\end{table}

The ablation studies confirm that both the SSIM loss and the Deep Feature Matching hypernetwork contribute significantly to the performance of our framework. The multi-resolution sampling and pose regularization also provide modest improvements.

\section{Code Samples}

\subsection{Registration Pipeline}
The main registration pipeline:

\begin{lstlisting}[language=Python]
def register_image(nerf_model, target_image, initial_pose, 
                  loss_fn, hypernetwork=None, 
                  num_iterations=500, lr=0.01):
    """
    Register a target image with a NeRF model.
    
    Args:
        nerf_model: Pre-trained NeRF model
        target_image: Target intraoperative image
        initial_pose: Initial camera pose estimate
        loss_fn: Loss function to use
        hypernetwork: Optional hypernetwork for style transfer
        num_iterations: Maximum number of iterations
        lr: Learning rate
        
    Returns:
        Optimized camera pose
        Loss history
        Rendered image at optimized pose
    """
    # Initialize pose parameters
    pose_params = torch.tensor(initial_pose, requires_grad=True)
    
    # Setup optimizer
    optimizer = torch.optim.Adam([pose_params], lr=lr)
    
    # Initialize loss history
    loss_history = []
    
    # Optimization loop
    for i in range(num_iterations):
        # Zero gradients
        optimizer.zero_grad()
        
        # Render image from current pose
        rendered_image = nerf_model.render(pose_params)
        
        # Apply hypernetwork if provided
        if hypernetwork is not None:
            style_code = hypernetwork.encode(target_image)
            style_params = hypernetwork(style_code)
            rendered_image = hypernetwork.apply_style(rendered_image, style_params)
        
        # Compute loss
        loss = loss_fn(rendered_image, target_image)
        
        # Backpropagate and update pose
        loss.backward()
        optimizer.step()
        
        # Record loss
        loss_history.append(loss.item())
        
        # Check convergence
        if i > 50 and abs(loss_history[-1] - loss_history[-50]) < 1e-5:
            break
    
    # Render final image
    with torch.no_grad():
        final_image = nerf_model.render(pose_params)
        if hypernetwork is not None:
            style_code = hypernetwork.encode(target_image)
            style_params = hypernetwork(style_code)
            final_image = hypernetwork.apply_style(final_image, style_params)
    
    return pose_params.detach(), loss_history, final_image
\end{lstlisting}

\subsection{Hypernetwork Implementation}
Implementation of the Deep Feature Matching hypernetwork:

\begin{lstlisting}[language=Python]
class DeepFeatureMatchingHypernetwork(nn.Module):
    """
    Hypernetwork for style transfer using deep feature matching.
    """
    def __init__(self, feature_extractor, output_dim):
        super().__init__()
        self.feature_extractor = feature_extractor
        
        # Freeze feature extractor
        for param in self.feature_extractor.parameters():
            param.requires_grad = False
        
        # Feature dimension from the feature extractor
        self.feature_dim = 512  # For VGG16 conv4_2
        
        # MLP to generate style parameters
        self.mlp = nn.Sequential(
            nn.Linear(self.feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim)
        )
    
    def encode(self, image):
        """Extract features from the image."""
        # Preprocess image if needed
        if image.shape[1] == 3:  # RGB
            # VGG expects normalized images
            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(image.device)
            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(image.device)
            image = (image - mean) / std
        
        # Extract features
        features = self.feature_extractor(image)
        
        # Global average pooling
        features = F.adaptive_avg_pool2d(features, (1, 1)).squeeze(-1).squeeze(-1)
        
        return features
    
    def forward(self, style_code):
        """Generate style parameters from style code."""
        return self.mlp(style_code)
    
    def apply_style(self, image, style_params):
        """Apply style parameters to the image."""
        # Split style parameters into scale and bias for RGB
        rgb_scale, rgb_bias = style_params.chunk(2, dim=1)
        
        # Reshape for broadcasting
        rgb_scale = rgb_scale.view(-1, 3, 1, 1)
        rgb_bias = rgb_bias.view(-1, 3, 1, 1)
        
        # Apply modulation
        styled_image = image * rgb_scale + rgb_bias
        
        # Ensure values are in valid range
        styled_image = torch.clamp(styled_image, 0, 1)
        
        return styled_image
\end{lstlisting}

\section{Dataset Details}

\subsection{Synthetic Dataset}
The synthetic dataset was generated using a pre-trained NeRF model of a brain surface, with the following specifications:

\begin{itemize}
    \item \textbf{Number of Images}: 100 target images
    \item \textbf{Resolution}: 256 Ã— 256 pixels
    \item \textbf{Camera Parameters}: 
    \begin{itemize}
        \item Focal Length: 35 mm
        \item Field of View: 60 degrees
        \item Distance from Brain Surface: 100-150 mm
    \end{itemize}
    \item \textbf{Perturbation Levels}:
    \begin{itemize}
        \item Small: 1-5 mm translation, 1-5 degrees rotation
        \item Medium: 5-10 mm translation, 5-10 degrees rotation
        \item Large: 10-20 mm translation, 10-20 degrees rotation
        \item Very Large: 20-30 mm translation, 20-30 degrees rotation
    \end{itemize}
\end{itemize}

\subsection{Real Dataset}
The real dataset consists of 20 cases from neurosurgical procedures, with the following characteristics:

\begin{itemize}
    \item \textbf{Preoperative MRI}:
    \begin{itemize}
        \item T1-weighted with gadolinium contrast
        \item 1 mm isotropic resolution
        \item Acquired 1-7 days before surgery
    \end{itemize}
    
    \item \textbf{Intraoperative Images}:
    \begin{itemize}
        \item Optical images captured with a surgical microscope
        \item Resolution: 1920 Ã— 1080 pixels
        \item Captured after dura opening and before tumor resection
    \end{itemize}
    
    \item \textbf{Ground Truth}:
    \begin{itemize}
        \item Manually annotated correspondences between MRI and intraoperative images
        \item 10-15 landmarks per case, marked by experienced neurosurgeons
        \item Estimated accuracy of manual annotations: 1-2 mm
    \end{itemize}
\end{itemize}

Due to privacy concerns, the real dataset cannot be publicly shared, but anonymized samples and statistical summaries are provided for reference. 